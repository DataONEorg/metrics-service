input {
	# /usr/share/logstash/bin/logstash -f test.conf < minimal.log
	
#	stdin { }

	beats {
		port => 5705
	}
}


filter {

	# Parse the JSON message into fields
	json {
		id => "message"
		source => "message"
		target => "parsedMessage"
		add_field => { "lsConfigVer" => 9 }
	}


	# Grab the fields of interest from the JSON
	mutate {
		id => "addFields"

		add_field => { "accessToken" => "%{[parsedMessage][accessToken]}" }
		add_field => { "method" => "%{[parsedMessage][method]}" }
		add_field => { "query" => "%{[parsedMessage][query]}" }
		add_field => { "referer" => "%{[parsedMessage][referer]}" }
		add_field => { "remoteIP" => "%{[parsedMessage][remoteIP]}" }
		add_field => { "request" => "%{[parsedMessage][request]}" }
		add_field => { "status" => "%{[parsedMessage][status]}" }
		add_field => { "timestamp" => "%{[parsedMessage][time]}" }
		add_field => { "userAgent" => "%{[parsedMessage][userAgent]}" }
		add_field => { "ver" => "%{[parsedMessage][ver]}" }

		add_tag => [ "addedFields" ]
	}


	# Drop events that aren't hitting the CN
	if [request] !~ /^\/cn\/v/ { 
		mutate {
			id => "dropNonCN"
			add_tag => [ "droppingNonCN" ]
		}

		drop { }
	}


	# URL Decode the query string
	urldecode {
		charset => "UTF-8"
		field => [ "query" ]
	}

	if [query] {
		kv {
			source => [ "query" ]
			field_split => "&?"
			target => [ "queryParts" ]
			add_tag => [ "splitQuery" ]
		}
	}

	# Drop queries that don't specify 25 rows returned
	if [queryParts] and [queryParts][rows] != "25" {
		mutate {
			id => "dropAutoQuery"
			add_tag => [ "droppingAutoQuery" ]
		}

		drop { }
	}

	# Elasticsearch doesn't support field names containing dots
#	if [queryParts][facet] {
		mutate {
			rename => { "[queryParts][facet.field]" => "[queryParts][facet_field]"
			            "[queryParts][facet.limit]" => "[queryParts][facet_limit]" 
			            "[queryParts][facet.mincount]" => "[queryParts][facet_mincount]"
			            "[queryParts][facet.missing]" => "[queryParts][facet_missing]"
			            "[queryParts][facet.query]" => "[queryParts][facet_query]"
			            "[queryParts][facet.range]" => "[queryParts][facet_range]"
			            "[queryParts][facet.range.end]" => "[queryParts][facet_range_end]"
			            "[queryParts][facet.range.gap]" => "[queryParts][facet_range_gap]"
			            "[queryParts][facet.range.start]" => "[queryParts][facet_range_start]"
			            "[queryParts][facet.sort]" => "[queryParts][facet_sort]"

			            "[queryParts][group.field]" => "[queryParts][group_field]"
			            "[queryParts][group.limit]" => "[queryParts][group_limit]"

			            "[queryParts][stats.facet]" => "[queryParts][stats_facet]"
			            "[queryParts][stats.field]" => "[queryParts][stats_field]"
			}
		}
#	}

	mutate {
		id => "removeQuery"
		remove_field => [ "query" ]
	}


	# Tag if IP is from a DataONE address
	cidr {
		id => "D1IP"
		address => [ "%{remoteIP}" ]
		network_path => "/etc/logstash/conf.d/dict/dataone_ips.txt"
		refresh_interval => 600
		add_tag => [ "dataone_ip", "ignore_ip" ]
	}

	# Tag if IP is in the known Robot IP list
	cidr {
		id => "ROBOTIP"
		address => [ "%{remoteIP}" ]
		network_path => "/etc/logstash/conf.d/dict/robot_ips.txt"
		refresh_interval => 600
		add_tag => [ "robot_ip", "ignore_ip" ]
	}


	# Tag if the user agent is on the COUNTER list
	translate {
		field => "userAgent"
		dictionary_path => "/etc/logstash/conf.d/dict/counter_ua.yml"
		destination => "[@metadata][robot]"
		fallback => "notCounterUA"
		exact => true
		regex => true
		refresh_interval => 86400   # 1 day = 86400 seconds
		add_tag => [ "translateCOUNTER" ]
	}

	# Tag if the user agent is on the machines list
	translate {
		field => "userAgent"
		dictionary_path => "/etc/logstash/conf.d/dict/machine_ua.yml"
		destination => "[@metadata][machine]"
		fallback => "notMachineUA"
		exact => true
		regex => true
		refresh_interval => 86400   # 1 day = 86400 seconds
		add_tag => [ "translateMachine" ]
	}


	# Georeference the IP address
	geoip {
		source => [remoteIP]
	}


	# Decode the Google Analytics cookie data
	if [parsedMessage][ga_cookie] and [parsedMessage][ga_cookie] != '-' {
		mutate {
			add_field => { "gaCookie" => "%{[parsedMessage][ga_cookie]}" }
			add_tag => [ "addedGA" ]
		}

		mutate {
			split => { "gaCookie" => "." }
			add_tag => [ "splitGA" ]
		}

		mutate {
			add_field => { "gaUserId" => "%{[gaCookie][2]}" }
			add_field => { "gaTimestampEpoch" => "%{[gaCookie][3]}" }
			remove_field => [ "gaCookie" ]
			add_tag => [ "parsedGA" ]
		}

		date {
			match => ["gaTimestampEpoch", "UNIX"]
			target => "gaTimestamp"
			remove_field => ["gaTimestampEpoch"]
	    }
	}


	if [parsedMessage][responseTime] {
		mutate {
			add_field => { "responseTime" => "%{[parsedMessage][responseTime]}" }
			add_tag => [ "addedResponseTime" ]
		}
		mutate {
			convert => { "responseTime" => "integer" }
		}
	}


	if [ver] {
	# Transfer request timestamp to @timestamp
	# Requests with ver < 1.2 were incorrectly reporting ET as UTC, so are convert here

		mutate {
			convert => { "ver" => "float" }
		}

		if [ver] >= 1.2 {
			date {
				match => ["timestamp", "ISO8601"]
				target => "@timestamp"
				remove_field => ["timestamp"]
				remove_field => ["ver"]
				add_tag => [ "timestampge12" ]
			}
		}
		else {
			date {
				match => ["timestamp", "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"]
				target => "@timestamp"
				timezone => "America/Los_Angeles"
				remove_field => ["timestamp"]
				remove_field => ["ver"]
				add_tag => [ "timestamplt12" ]
			}
		}

	}


	# Decode the JSON web token
	ruby {
		init => "require 'base64'"
		code => "
				encoded = event.get('accessToken') 
				if encoded != '-'
					decoded = Base64.decode64(encoded.match(/\.([^\.]+)\./)[0])
    				event.set('jwt', decoded)
    			end
    			"
    	remove_field => [ "accessToken" ]
	}

	# Parse the JWT
	json {
		id => "jwt"
		source => "jwt"
		target => "parsedJwt"
	}

	# Extract fields and remove the JWT
	if [jwt] {
		mutate {
			add_field => {"jwtFullName" => "%{[parsedJwt][fullName]}"}
			add_field => {"jwtUserId" => "%{[parsedJwt][userId]}"}
			add_field => {"jwtIssuedAt" => "%{[parsedJwt][issuedAt]}"}
			remove_field => [ "jwt", "parsedJwt" ]
			add_tag => [ "addedJWT" ]
		}
	}



	# Compute the session ID
	mutate {
		add_field => { "userId" => "%{[remoteIP]} %{[userAgent]}" }
		add_tag => [ "ipUaSessionId" ]
	}

	if [userId] {
		fingerprint {
			method => "MD5"
			source => "userId"
			target => "userIdHash"
		}
	}
	
	mutate {
		add_field => { "sessionId" => "%{userIdHash}|%{+Y}%{+MM}%{+dd}%{+HH}" }
		#remove_field => [ "userId", "userIdHash" ]
	}


	# Clean up some extra fields
	mutate {
		#remove_field => [ "message", "parsedMessage", "host" ]
		remove_field => [ "accessToken", "host" ]
	}

}


output {
#	stdout { codec => rubydebug { metadata => true } }
#
#	file {
#		path => "/home/flathers/logproc/logstashout.txt"
#	}

	elasticsearch {
	    hosts => ["127.0.0.1:9200"]
	    index => "logstash-test2"
	    #document_type => "logevent"
	}
}
