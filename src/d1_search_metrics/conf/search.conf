input {

#	stdin { 
#		id => "standardInput"
#		add_field => { "recordType" => "eventlog" }
#	}

	# events coming in on port 5705 are search events
	beats {
		id => "search"
		port => 5705

		add_field => { "recordType" => "search" }
		add_field => { "event" => "search" }
	}

	# events coming in on port 5704 are eventlog events
	beats {
		id => "eventlog"
		port => 5704
		ssl => true
		ssl_certificate => "/etc/logstash/lumberjack.cert"
		ssl_key => "/etc/logstash/lumberjack.key"

		add_field => { "recordType" => "eventlog" }
	}
}


filter {

	# Parse the JSON message into fields
	json {
		id => "message"
		source => "message"
		target => "parsedMessage"
		add_field => { "lsConfigVer" => 16 }
	}


	# processing steps for apache search events
	if [recordType] == "search" {
	
		# grab the fields of interest from the JSON
		mutate {
			id => "addSearchFields"

			add_field => { "accessToken" => "%{[parsedMessage][accessToken]}" }
			add_field => { "method" => "%{[parsedMessage][method]}" }
			add_field => { "query" => "%{[parsedMessage][query]}" }
			add_field => { "referer" => "%{[parsedMessage][referer]}" }
			add_field => { "remoteIP" => "%{[parsedMessage][remoteIP]}" }
			add_field => { "request" => "%{[parsedMessage][request]}" }
			add_field => { "status" => "%{[parsedMessage][status]}" }
			add_field => { "timestamp" => "%{[parsedMessage][time]}" }
			add_field => { "userAgent" => "%{[parsedMessage][userAgent]}" }
			add_field => { "ver" => "%{[parsedMessage][ver]}" }

			add_tag => [ "addedSearchFields" ]
		}


		# drop apache log entries that aren't accessing the CN
		if [request] !~ /^\/cn\/v/ { 
			mutate {
				id => "dropNonCN"
				add_tag => [ "droppingNonCN" ]
			}

			drop { }
		}

		# URL decode the query string
		urldecode {
			charset => "UTF-8"
			field => [ "query" ]
		}

		# split the decoded query string into an array
		if [query] {
			kv {
				source => [ "query" ]
				field_split => "&?"
				target => [ "queryParts" ]
				add_tag => [ "splitQuery" ]
			}
		}

		# the web search interface requests 25 rows at a time
		# for search results, but other values for tasks such
		# as populating facet fields, so drop queries that don't
		# specify 25 rows returned
		if [queryParts] and [queryParts][rows] != "25" {
			mutate {
				id => "dropAutoQuery"
				add_tag => [ "droppingAutoQuery" ]
			}

			drop { }
		}

		# the query field names contain dots, but elasticsearch
		# doesn't support field names containing dots, so replace
		# with underscores -- there are more automated ways to do
		# this, but the docs warn that they are expensive
		mutate {
			rename => { "[queryParts][facet.field]" => "[queryParts][facet_field]"
			            "[queryParts][facet.limit]" => "[queryParts][facet_limit]" 
			            "[queryParts][facet.mincount]" => "[queryParts][facet_mincount]"
			            "[queryParts][facet.missing]" => "[queryParts][facet_missing]"
			            "[queryParts][facet.query]" => "[queryParts][facet_query]"
			            "[queryParts][facet.range]" => "[queryParts][facet_range]"
			            "[queryParts][facet.range.end]" => "[queryParts][facet_range_end]"
			            "[queryParts][facet.range.gap]" => "[queryParts][facet_range_gap]"
			            "[queryParts][facet.range.start]" => "[queryParts][facet_range_start]"
			            "[queryParts][facet.sort]" => "[queryParts][facet_sort]"

			            "[queryParts][group.field]" => "[queryParts][group_field]"
			            "[queryParts][group.limit]" => "[queryParts][group_limit]"

			            "[queryParts][stats.facet]" => "[queryParts][stats_facet]"
			            "[queryParts][stats.field]" => "[queryParts][stats_field]"
			}
		}


		# decode the Google Analytics cookie data
		if [parsedMessage][ga_cookie] and [parsedMessage][ga_cookie] != '-' {
			mutate {
				add_field => { "gaCookie" => "%{[parsedMessage][ga_cookie]}" }
				add_tag => [ "addedGA" ]
			}

			mutate {
				split => { "gaCookie" => "." }
				add_tag => [ "splitGA" ]
			}

			mutate {
				add_field => { "gaUserId" => "%{[gaCookie][2]}" }
				add_field => { "gaTimestampEpoch" => "%{[gaCookie][3]}" }
				remove_field => [ "gaCookie" ]
				add_tag => [ "parsedGA" ]
			}

			date {
				match => ["gaTimestampEpoch", "UNIX"]
				target => "gaTimestamp"
				remove_field => ["gaTimestampEpoch"]
		    }
		}


		# some of the apache log entries don't contain response times
		if [parsedMessage][responseTime] {
			mutate {
				add_field => { "responseTime" => "%{[parsedMessage][responseTime]}" }
				add_tag => [ "addedResponseTime" ]
			}
			mutate {
				convert => { "responseTime" => "integer" }
			}
		}


		if [ver] {
		# replace @timsestamp with the request timestamp
		# requests with ver < 1.2 were incorrectly reporting ET as UTC, so we convert here

			mutate {
				convert => { "ver" => "float" }
			}

			if [ver] >= 1.2 {
				date {
					match => ["timestamp", "ISO8601"]
					target => "@timestamp"
					remove_field => ["timestamp"]
					remove_field => ["ver"]
					add_tag => [ "timestampge12" ]
				}
			}
			else {
				date {
					match => ["timestamp", "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"]
					target => "@timestamp"
					timezone => "America/Los_Angeles"
					remove_field => ["timestamp"]
					remove_field => ["ver"]
					add_tag => [ "timestamplt12" ]
				}
			}
		}


		# decode the JSON web token
		ruby {
			init => "require 'base64'"
			code => "
					encoded = event.get('accessToken') 
					if encoded != '-'
						decoded = Base64.decode64(encoded.match(/\.([^\.]+)\./)[0])
	    				event.set('jwt', decoded)
	    			end
	    			"
	    	remove_field => [ "accessToken" ]
		}

		# parse the JWT
		json {
			id => "jwt"
			source => "jwt"
			target => "parsedJwt"
		}

		# extract fields and remove the JWT
		if [jwt] {
			mutate {
				add_field => {"jwtFullName" => "%{[parsedJwt][fullName]}"}
				add_field => {"jwtUserId" => "%{[parsedJwt][userId]}"}
				add_field => {"jwtIssuedAt" => "%{[parsedJwt][issuedAt]}"}
				remove_field => [ "jwt", "parsedJwt" ]
				add_tag => [ "addedJWT" ]
			}
		}

	}


	# processing steps for eventlog events
	if [recordType] == "eventlog" {

		# drop events originating from 127.0.0.1
		# some pre-production search events come through on port 5704, so drop them
		if [parsedMessage][ipAddress] == "127.0.0.1" or [parsedMessage][beat][name] == "search" { 
			mutate {
				id => "dropNoiseEvent"
				add_tag => [ "droppingNoiseEvents" ]
			}

			drop { }
		}

		# grab the fields of interest from the JSON
		mutate {
			id => "addEventlogFields"

			add_field => { "dateAggregated" => "%{[parsedMessage][dateAggregated]}" }
			add_field => { "dateLogged" => "%{[parsedMessage][dateLogged]}" }
			add_field => { "entryId" => "%{[parsedMessage][entryId]}" }
			add_field => { "event" => "%{[parsedMessage][event]}" }
			add_field => { "formatId" => "%{[parsedMessage][formatId]}" }
			add_field => { "formatType" => "%{[parsedMessage][formatType]}" }
			add_field => { "isPublic" => "%{[parsedMessage][isPublic]}" }
			add_field => { "nodeEntryId" => "%{[parsedMessage][id]}" }
			add_field => { "nodeId" => "%{[parsedMessage][nodeId]}" }
			add_field => { "pid" => "%{[parsedMessage][pid]}" }
			add_field => { "readPermission" => "%{[parsedMessage][readPermission]}" }
			add_field => { "remoteIP" => "%{[parsedMessage][ipAddress]}" }
			add_field => { "rightsHolder" => "%{[parsedMessage][rightsHolder]}" }
			add_field => { "size" => "%{[parsedMessage][size]}" }
			add_field => { "subject" => "%{[parsedMessage][subject]}" }
			add_field => { "userAgent" => "%{[parsedMessage][userAgent]}" }
			add_field => { "versionCompliance" => "%{[parsedMessage][versionCompliance]}" }

			add_tag => [ "addedEventlogFields" ]
		}

		# parse the date fields

		date {
			match => ["dateAggregated", "ISO8601"]
			target => "dateAggregated"
		}

		date {
			match => ["dateLogged", "ISO8601"]
			target => "@timestamp"
			remove_field => ["dateLogged"]
		}
	}


	# processing steps for all events

	# tag if IP is from a DataONE address
	cidr {
		id => "D1IP"
		address => [ "%{remoteIP}" ]
		network_path => "/etc/logstash/conf.d/dict/dataone_ips.txt"
		refresh_interval => 600
		add_tag => [ "dataone_ip", "ignore_ip" ]
	}

	# tag if IP is in the known Robot IP list
	cidr {
		id => "ROBOTIP"
		address => [ "%{remoteIP}" ]
		network_path => "/etc/logstash/conf.d/dict/robot_ips.txt"
		refresh_interval => 600
		add_tag => [ "robot_ip", "ignore_ip" ]
	}


	# tag if the user agent is on the COUNTER list
	translate {
		field => "userAgent"
		dictionary_path => "/etc/logstash/conf.d/dict/counter_ua.yml"
		destination => "[@metadata][robot]"
		fallback => "notCounterUA"
		exact => true
		regex => true
		refresh_interval => 86400   # 1 day = 86400 seconds
		add_tag => [ "translateCOUNTER" ]
	}

	# tag if the user agent is on the machines list
	translate {
		field => "userAgent"
		dictionary_path => "/etc/logstash/conf.d/dict/machine_ua.yml"
		destination => "[@metadata][machine]"
		fallback => "notMachineUA"
		exact => true
		regex => true
		refresh_interval => 86400   # 1 day = 86400 seconds
		add_tag => [ "translateMachine" ]
	}


	# georeference the IP address
	geoip {
		source => [remoteIP]
	}

	# compute the session ID
	mutate {
		add_field => { "userId" => "%{[remoteIP]} %{[userAgent]}" }
		add_tag => [ "ipUaSessionId" ]
	}

	if [userId] {
		fingerprint {
			method => "MD5"
			source => "userId"
			target => "userIdHash"
		}
	}
	
	mutate {
		add_field => { "sessionId" => "%{userIdHash}|%{+Y}%{+MM}%{+dd}%{+HH}" }
		remove_field => [ "userId" ]
	}


	# clean up some extra fields
	mutate {
		remove_field => [ "message", "parsedMessage", "host" ]
	}

}


output {
#	stdout { codec => rubydebug { metadata => true } }
#
#	file {
#		path => "/home/flathers/logproc/logstashout.txt"
#	}

	elasticsearch {
	    hosts => ["127.0.0.1:9200"]
	    index => "logstash-test2"
	}
}
